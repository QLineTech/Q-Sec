# LangChain + LLM Entegrasyonu 



![](https://markovate.com/wp-content/uploads/2023/06/Leveraging-LangChain-for-Next-Gen-Language-Models.webp)

## 📚 LangChain Nedir?
LangChain, büyük dil modellerini (LLM) uygulamalarla entegre etmek için kullanılan açık kaynak bir framework'tür. 2022'de Harrison Chase tarafından oluşturulmuş ve kısa sürede AI geliştirme dünyasının vazgeçilmez araçlarından biri haline gelmiştir.



### 🌟 Temel Özellikleri

LangChain, karmaşık AI görevlerini basit yapı taşlarına bölerek yönetilebilir hale getirir. Düşünün ki bir LEGO seti gibi - her parça kendi başına basit, ama birleştirdiğinizde güçlü sistemler oluşturabiliyorsunuz.



![](https://img.freepik.com/premium-photo/digital-chat-bot-are-assistant-conversation_587448-3195.jpg)



### 💻 Kaynak ve Dokümanlar

- GitHub: https://github.com/langchain-ai/langchain
- Resmi Dokümantasyon: https://python.langchain.com/
- Topluluk: https://discord.gg/6adMQxSpJS



## 🛠️ Kurulum ve Başlangıç

Modern siber güvenlik uzmanları için LangChain ile çalışmaya başlamanın iki yolu var. Hadi bunları inceleyelim:

![AI Applications](https://www.hpcwire.com/wp-content/uploads/2024/01/Groq_x_aiXplain.jpg){width="350px"} 

------------

##### AI dedigimiz ChatBot degil gercek kullanimi cok daha farkli 

##### AI Technology Beyond Chatbots 



Ben bir bilgisayar mühendisiyim. 

Bu teknoloji, sadece ChatGPT'de yazı yazmaktan çok daha derin.

Bilgisayar bilimcileri olarak biz, bu teknolojinin motor kısmını anlıyoruz. Kodun nasıl düşündüğünü biliyoruz.

Sıradan bir kullanıcı sadece sohbet ederken, biz sistemler tasarlıyoruz. Algoritmaları eğitiyoruz. Veriyi işliyoruz.

Şimdi size göstereceğim örnekler, aradaki bu derin farkı net olarak ortaya koyacak.

Çünkü biz sadece kullanıcı değil, yaratıcıyız. Bu teknolojinin mimarlarıyız.

Ve siz, değerli öğrencilerim, bu yolculukta benimle birlikte olacaksınız.

Hazırsanız başlayalım.

-------------------------------

### 1. Groq ile Ücretsiz Başlangıç
```python
# Gerekli kütüphanelerin kurulumu
pip install langchain groq
```



## 📌 Ön Gereksinimler

> "Büyük bir yolculuk tek bir adımla başlar" - Lao Tzu

| Gereksinim             | Açıklama                                 | Öncelik  |
| ---------------------- | ---------------------------------------- | -------- |
| Groq Hesabı            | api.groq.com üzerinden ücretsiz kayıt    | Zorunlu  |
| API Anahtarı           | Panel üzerinden oluşturulacak            | Zorunlu  |
| cURL                   | Komut satırı aracı (çoğu sistemde yüklü) | Zorunlu  |
| Temel Terminal Bilgisi | Basit komutları çalıştırabilme           | Önerilen |



## 🎯 İlk Adımlarımız

### 1. API Anahtarı Oluşturma
```markdown
1. Groq paneline giriş yapın
2. "API Keys" bölümünü bulun
3. "Create New Key" butonuna tıklayın
4. Anahtarınızı güvenli bir yere kaydedin
```



### 2. İlk Kodumuzu Yazalım

```python
curl https://api.groq.com/openai/v1/chat/completions -s \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $GROQ_API_KEY" \
-d '{
"model": "llama-3.3-70b-versatile",
"messages": [{
    "role": "user",
    "content": "siber guvenliginden bilgin var mi?"
}]
}'
```





### 3. Önemli Noktalar 🔍

* `$GROQ_API_KEY` kısmını kendi API anahtarınızla değiştirmelisiniz
* API anahtarı asla başkalarıyla paylaşılmamalıdır
* Modeli değiştirmek isterseniz "model" parametresini güncelleyebilirsiniz
* İstediğiniz soruyu "content" kısmına yazabilirsiniz

## ⏱️ Pratik Zamanı

> **Göreviniz**: Yukarıdaki kodu kendi API anahtarınızla çalıştırmak ve siber güvenlik hakkında bir soru sormak.

**Süre**: 10 dakika



# Python ile Groq API Kullanım Örnekleri 🐍

Sevgili öğrencilerim, aynı API çağrısını 3 farklı Python kütüphanesi kullanarak nasıl yapacağımızı öğrenelim. Her yöntemin kendine özgü avantajları var.

## 1. Requests Kütüphanesi ile Kullanım
```python
import requests
import json

# API bilgilerimizi tanımlayalım
API_KEY = "sizin_api_keyiniz"
API_URL = "https://api.groq.com/openai/v1/chat/completions"

# İstek başlıklarımızı hazırlayalım
headers = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {API_KEY}"
}

# Göndereceğimiz veriyi hazırlayalım
data = {
    "model": "llama-3.3-70b-versatile",
    "messages": [{
        "role": "user",
        "content": "siber guvenliginden bilgin var mi?"
    }]
}

# API çağrısını yapalım ve sonucu alalım
response = requests.post(API_URL, headers=headers, json=data)

# Yanıtı işleyelim
if response.status_code == 200:
    result = response.json()
    print(result['choices'][0]['message']['content'])
else:
    print(f"Hata: {response.status_code}")
    print(response.text)
```

## 2. Groq Kütüphanesi ile Kullanım
```python
from groq import Groq

# Groq istemcimizi oluşturalım
client = Groq(
    api_key="sizin_api_keyiniz"
)

# Sohbet tamamlama isteği yapalım
chat_completion = client.chat.completions.create(
    messages=[{
        "role": "user",
        "content": "siber guvenliginden bilgin var mi?"
    }],
    model="llama-3.3-70b-versatile",
)

# Yanıtı yazdıralım
print(chat_completion.choices[0].message.content)
```

## 3. OpenAI Kütüphanesi ile Kullanım
```python
from openai import OpenAI

# İstemciyi başlatalım
client = OpenAI(
    api_key="sizin_api_keyiniz",
    base_url="https://api.groq.com/openai/v1"  # Groq API endpoint'ini belirtiyoruz
)

# Sohbet tamamlama isteği yapalım
response = client.chat.completions.create(
    model="llama-3.3-70b-versatile",
    messages=[{
        "role": "user",
        "content": "siber guvenliginden bilgin var mi?"
    }]
)

# Yanıtı yazdıralım
print(response.choices[0].message.content)
```



## 🔍 Önemli Notlar:



1. **Requests Kütüphanesi**:
   - En temel yöntem
   - Her Python kurulumunda genellikle mevcut
   - Ham HTTP istekleri üzerinde tam kontrol sağlar
   - Hata yönetimi manuel yapılmalı

2. **Groq Kütüphanesi**:
   - Groq'un resmi kütüphanesi
   - Daha temiz ve özelleştirilmiş bir API
   - Type hints ve otomatik tamamlama desteği
   - `pip install groq` ile kurulur

3. **OpenAI Kütüphanesi**:
   - OpenAI benzeri bir arayüz
   - Groq'un OpenAI-uyumlu API'sini kullanır
   - OpenAI'dan Groq'a geçiş yapanlar için ideal
   - `pip install openai` ile kurulur



![](https://c4.wallpaperflare.com/wallpaper/1014/266/488/python-programming-blue-green-wallpaper-preview.jpg)



## 💡 Kullanım Önerileri

Her üç yöntem de aynı sonucu verecektir, ancak:
- Basit projeler için **requests** 
- Groq-spesifik projeler için **groq**
- OpenAI'dan geçiş yapıyorsanız **openai** kütüphanesini tercih edebilirsiniz.



## ⚠️ Güvenlik Hatırlatması

API anahtarınızı kodunuzda doğrudan yazmak yerine:
```python
import os
API_KEY = os.getenv("GROQ_API_KEY")
```
şeklinde çevre değişkenlerinden almanızı öneririm.



### 🎯 Başarı Kriterleri:
1. API anahtarının doğru yerleştirilmesi
2. Kodun hatasız çalışması
3. yanıt alınması



## 💡 İpuçları

* Terminal/komut istemini yönetici olarak çalıştırmanız gerekebilir
* API anahtarını kopyalarken boşluk bırakmamaya dikkat edin
* Türkçe karakterlerde sorun yaşarsanız, Unicode encoding kullanın



## 🤔 Sık Sorulan Sorular

> "API anahtarım çalışmıyor, ne yapmalıyım?"
* Anahtarın tam olarak kopyalandığından emin olun
* Boşluk ve özel karakterleri kontrol edin
* Hesabınızın aktif olduğundan emin olun

---



## 🚀 Groq Entegrasyonu



Fiyatlandırma konusuna gelince, her AI şirketi kendi iş modelini ve stratejisini belirler. Groq'un ücretsiz hizmet sunması, pazara giriş stratejisinin bir parçası olabilir. Bu yaklaşım:

- Kullanıcı tabanı oluşturmaya
- Pazar payı kazanmaya
- Ürünlerini test edip geliştirmeye yardımcı olur

Diğer şirketler (Anthropic, OpenAI gibi) ise:
- AI modellerinin eğitimi ve çalıştırılması için gereken yüksek maliyetleri
- Araştırma ve geliştirme yatırımlarını
- Altyapı giderlerini

karşılamak için ücretli hizmet sunmayı tercih edebilirler.

Bu farklı yaklaşımların her birinin kendi avantajları ve dezavantajları vardır. Uzun vadede hangi stratejinin daha başarılı olacağını zaman gösterecektir.



![](https://mychessets.com/cdn/shop/articles/text-to-image_4eb264d3-9037-4ffc-a7aa-7582cd4f40d3.png?v=1714752472)



# Yapay Zeka Modelleri 

##### 						Donanım Altyapısı Detaylı İnceleme







## 1. Büyük Dil Modelleri ve Parametreler

### Açık Kaynak Modeller
| Model        | Parametre | Min. VRAM | Önerilen GPU | Tahmini Maliyet | Özel Gereksinimler                 |
| ------------ | --------- | --------- | ------------ | --------------- | ---------------------------------- |
| LLaMA 3 70B  | 70 milyar | 140GB+    | 4x A100 80GB | $50,000+        | Multi-GPU setup, NVLink bağlantısı |
| LLaMA 3 34B  | 34 milyar | 70GB+     | 2x A100 80GB | $25,000+        | Yüksek hızlı PCIe bağlantısı       |
| LLaMA 3 13B  | 13 milyar | 28GB+     | 1x A100 40GB | $12,000+        | CUDA 11.8+                         |
| LLaMA 3 8B   | 8 milyar  | 16GB+     | RTX 4090     | $2,000+         | NVIDIA GPU (min. Ampere)           |
| Mixtral 8x7B | 56 milyar | 110GB+    | 2x A100 80GB | $25,000+        | MoE için özel bellek yönetimi      |



## Detaylı Teknik Açıklama

### Bellek Hesaplama Formülü
```
Minimum VRAM = (Parametre Sayısı × 2 × Precision) + Ara Bellek
- Parametre Sayısı: Modelin milyar cinsinden boyutu
- Precision: FP16 için 2 byte, INT8 için 1 byte
- Ara Bellek: Yaklaşık model boyutunun %20'si
```





![](https://photos.peopleimages.com/picture/202304/2809796-brain-pattern-ai-generated-and-digital-graphic-of-intelligence-and-neuroscience.-isolated-dark-background-and-no-people-with-neuro-connection-and-futuristic-artificial-intelligence-data-fit_400_400.jpg)



# Bu parametre nedir ?    70 milyar, ...



Yapay zeka modellerindeki parametreler, modelin eğitim sırasında öğrendiği ve ayarladığı sayısal değerlerdir - tıpkı bir öğrencinin beyni nasıl yeni bilgileri öğrenirken nöronlar arasındaki bağlantıları güçlendirip zayıflatıyorsa, AI modelinin de milyarlarca sayısal "ağırlığı" öğrenme sürecinde ayarlanır ve bu sayısal değerler modelin bilgi ve yeteneklerini temsil eder.



**Parametreler, yapay zeka modelinin öğrenme sürecinde ayarladığı ve tüm "bilgi birikimini" içeren sayısal değerlerdir.**





### Optimizasyon Teknikleri

1. **Quantization (Nicemleme)**
   - INT8: Bellek kullanımını yarıya düşürür
   - INT4: Bellek kullanımını dörtte bire düşürür
   - Performans-doğruluk dengesi gözetilmeli

2. **Model Sharding (Bölümleme)**
   ```python
   # Örnek kod: Model bölümleme
   model = LlamaForCausalLM.from_pretrained(
       "meta-llama/Llama-70b",
       device_map="auto",  # Otomatik GPU dağıtımı
       load_in_8bit=True   # INT8 quantization
   )
   ```

### Sistem Gereksinimleri Detayları

#### LLaMA 3 70B için Tam Sistem
- CPU: AMD EPYC 7763 veya Intel Xeon Platinum
- RAM: 256GB+ ECC DDR4/5
- Depolama: 2TB+ NVMe SSD (PCIe 4.0)
- Güç Kaynağı: 3000W+ (80+ Platinum)
- Soğutma: Sıvı soğutma veya yüksek performanslı hava soğutma





#### Mixtral 8x7B için Sistem

```python
# Sistem kontrolü için örnek kod
def check_system_requirements():
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
    if gpu_memory < 110:
        raise ValueError(f"GPU memory ({gpu_memory}GB) insufficient for Mixtral")
```





### Maliyet Optimizasyonu

1. **Bulut Çözümleri**
   - AWS p4d.24xlarge: ~$32/saat
   - Google Cloud A2-ultragpu: ~$28/saat
   - Azure NC A100 v4: ~$30/saat



# ❓ Bu Rakamlar Çok Mu Büyük ? Hayır ❓

# 

![![](https://datacentrenews.uk/uploads/story/2023/10/30/nvidia.webp)](https://datacentrenews.uk/uploads/story/2023/10/30/nvidia.webp)

### Elon Musk'un 19 Günlük GPU Kurulum Rekoru

Nvidia CEO'su Jensen Huang, Elon Musk ve xAI ekibinin olağanüstü bir mühendislik başarısını paylaştı. Normal şartlarda 4 yıl süren bir sürecin sadece 19 günde tamamlanması teknoloji dünyasında büyük yankı uyandırdı.

## Projenin Detayları

Proje kapsamında **100.000 adet** **Nvidia H200 Blackwell GPU**'nun kurulumu gerçekleştirildi. Bu süreçte sıvı soğutma sistemleri ve güç altyapısının kurulumu da dahil olmak üzere tüm tesis sıfırdan inşa edildi. Jensen Huang'a göre bu tür bir projenin normal şartlarda 3 yılı planlama, 1 yılı kurulum olmak üzere toplam 4 yıl sürmesi beklenirdi.

## Maliyet ve Kapasite Analizi

H200 GPU'ların her biri yaklaşık 40.000$ değerinde olup, toplam donanım maliyeti **4 milyar doları** bulmaktadır. Her bir GPU 141GB VRAM kapasitesine sahip olduğundan, **toplam sistem 14.1 Petabyte VRAM sunmaktadır.**





## Potansiyel Kullanım Örnekleri

1. **Büyük Dil Modeli Eğitimi**: 

   Bu altyapı, GPT-4 ölçeğinde **(1.8 trilyon parametre) bir modeli 24 saat içinde eğitebilir.** Normal şartlarda bu süreç haftalarca sürebilmektedir.



1. **Bilimsel Simülasyonlar**: 

   İklim değişikliği modellemelerinde, 100 yıllık küresel iklim değişikliği senaryolarını sadece birkaç saat içinde simüle edebilir. Geleneksel sistemlerde bu işlem aylarca sürebilir.

   

2. **Görüntü İşleme**: 

   Tüm Instagram fotoğraf arşivinin (yaklaşık 50 milyar fotoğraf) yapay zeka ile analizi ve sınıflandırılması işlemini 48 saat içinde tamamlayabilir. Bu işlem normal sistemlerde yıllar alabilirdi.

-------------------------



Bu bilgiler ışığında, model seçimi yaparken şunları göz önünde bulundurmalısınız:

1. Kullanım amacınız
2. Mevcut donanım altyapınız
3. Bütçe kısıtlamalarınız
4. Ölçeklendirme ihtiyaçlarınız

Her model için optimum performans almak üzere sistem yapılandırması farklılık gösterecektir.



### Kapalı Kaynak Modeller

-----

| Model Ailesi | Model Versiyonu | Parametre Sayısı | Context Window | Özel Özellikler                                              |
| ------------ | --------------- | ---------------- | -------------- | ------------------------------------------------------------ |
| OpenAI       | GPT-4           | ~1.8 trilyon     | 128K token     | • Fine-tuning desteği ile özelleştirilebilir                 |
| Anthropic    | Claude 3 Opus   | Açıklanmadı      | 200K token     | • En güçlü ve kapsamlı versiyon<br>• Karmaşık görevler için optimize edilmiş |
|              | Claude 3 Sonnet | Açıklanmadı      | 200K token     | • Denge odaklı versiyon<br>• Günlük kullanım için optimize   |
|              | Claude 3 Haiku  | Açıklanmadı      | 200K token     | • Hız odaklı versiyon<br>• Hızlı yanıt süreleri              |
| Google       | Gemini Ultra    | Açıklanmadı      | 32K token      | • Google'ın en gelişmiş modeli<br>• Çoklu modalite desteği   |
|              | Gemini Pro      | Açıklanmadı      | 32K token      | • Orta segment kullanıcılar için<br>• Dengeli performans     |
|              | Gemini Nano     | Açıklanmadı      | 32K token      | • Mobil cihazlar için optimize<br>• Düşük kaynak tüketimi    |

Bu tablo, her bir model ailesinin temel özelliklerini ve farklılıklarını göstermektedir. Dikkat edilmesi gereken önemli bir nokta, birçok modelin parametre sayısının resmi olarak açıklanmamış olmasıdır. Ayrıca, her modelin kendine özgü güçlü yönleri ve kullanım senaryoları bulunmaktadır.



## 2. Eğitim Süreci ve Veri Boyutları

### Veri Boyutları
- GPT-4: 13 trilyon token (~45TB metin)
- LLaMA 2: 2 trilyon token (~7TB metin)
- Claude 3: Constitutional AI ile özel veri seti
- PaLM 2: 3.6 trilyon token (~12TB metin)

### Eğitim Süreleri
- Büyük modeller (70B+): 3-6 ay

- Orta boy modeller (13B-34B): 1-3 ay

- Küçük modeller (7B-): 2-4 hafta

  

## 3. GPU vs CPU vs Kuantum Bilgisayar

### GPU Avantajları
- Paralel işlem yeteneği
- Matris çarpımlarında yüksek performans
- CUDA gibi AI odaklı kütüphaneler
- Yüksek bellek bant genişliği

### CPU Özellikleri
- Sıralı işlemlerde daha iyi
- Genel amaçlı kullanım
- Karmaşık kontrol yapıları
- Daha düşük paralel işlem gücü



GPU ve CPU karşılaştırmasını bir tablo halinde organize edelim. Bu iki önemli işlem biriminin temel özelliklerini yan yana görerek aralarındaki farkları daha iyi anlayabiliriz:



| Özellik             | GPU (Grafik İşlem Birimi)                                    | CPU (Merkezi İşlem Birimi)                                   |
| ------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| İşlem Mimarisi      | Binlerce küçük işlem çekirdeği ile paralel işlem yapabilir   | Az sayıda güçlü çekirdek ile sıralı işlemlerde uzmanlaşmış   |
| Matematik İşlemleri | Matris çarpımları ve vektör işlemlerinde üstün performans gösterir | Karmaşık matematiksel işlemleri sıralı olarak gerçekleştirir |
| Yazılım Desteği     | CUDA gibi AI ve grafik odaklı özel kütüphaneler sunar        | Genel amaçlı programlama için geniş yazılım ekosistemi       |
| Bellek Yönetimi     | Yüksek bellek bant genişliği ile büyük veri setlerini hızlı işler | Daha düşük bellek bant genişliği ama daha esnek bellek yönetimi |
| Uygulama Alanı      | Yapay zeka eğitimi, görüntü işleme, grafik render işlemleri  | Sistem yönetimi, ofis uygulamaları, web tarama               |
| Kontrol Yapıları    | Basit ve tekrarlayan işlemler için optimize edilmiş          | Karmaşık karar yapıları ve dallanmalar için ideal            |



Bu karşılaştırma bize gösteriyor ki, GPU ve CPU birbirlerinin alternatifi değil, tamamlayıcısıdır. 

Modern bilgisayar sistemleri, her iki işlem biriminin güçlü yanlarından faydalanacak şekilde tasarlanmıştır. Örneğin, yapay zeka uygulamalarında GPU'lar ağır matematiksel hesaplamaları yaparken, CPU'lar sistem kontrolü ve veri yönetimi görevlerini üstlenir. 



Bu sinerji, özellikle derin öğrenme ve büyük veri işleme gibi karmaşık uygulamalarda optimal performans sağlar.



